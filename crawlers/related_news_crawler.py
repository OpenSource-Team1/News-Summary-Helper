import feedparser
import time, datetime
import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../")))
from utils.connection import updateDB
from newspaper import Article
from sqlalchemy import text

# ğŸ“Œ ì˜ˆì‹œ: ë„¤ì´ë²„ ë‰´ìŠ¤ RSS (IT/ê³¼í•™)
rss_url = "	http://www.khan.co.kr/rss/rssdata/total_news.xml"  # ì˜ˆì‹œ (ì „ìì‹ ë¬¸)

# 1ï¸âƒ£ RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ URL ê°€ì ¸ì˜¤ê¸°
feed = feedparser.parse(rss_url)
print("RSS URL:", rss_url)

print("Feed keys:", feed.keys())  # feed.entries ì™¸ì— ì–´ë–¤ í‚¤ê°€ ìˆëŠ”ì§€ ë³´ê¸°
print("Entries ê°œìˆ˜:", len(feed.entries))

print(f"RSSì—ì„œ {len(feed.entries)}ê°œì˜ ê¸°ì‚¬ ë°œê²¬!")

# 2ï¸âƒ£ ê° ê¸°ì‚¬ í¬ë¡¤ë§
sqlList = []
paramList = []
for entry in feed.entries:
    url = entry.link
    print(f"\n[ê¸°ì‚¬ URL] {url}")

    try:
        # newspaper3k ì‚¬ìš©
        article = Article(url, language='ko')
        article.download()
        article.parse()


        print(f"ì œëª©: {article.title}")
        print(f"ë‚ ì§œ: {str(article.publish_date)[:10]}")
        print(f"ë³¸ë¬¸ (10ì): {article.text[:10]}...")

        # âœ… ì—¬ê¸°ì„œ DB ì €ì¥ or íŒŒì¼ ì €ì¥ ê°€ëŠ¥ (ì˜ˆ: CSV, SQLite ë“±)
        sql = text("""
        INSERT INTO related_news (url, title, date, content)
        VALUES (:url, :title, :date, :content)
        """)

        params = {
            "url": entry.link,
            "title": article.title,
            "date": str(article.publish_date)[:10],
            "content": article.text
        }

       # ê¸ˆì¼ ê¸°ì‚¬ë§Œ í¬ë¡¤ë§í•˜ë„ë¡ í•„í„°ë§
        if str(article.publish_date)[:10] == str(datetime.datetime.now())[:10]:
            print('this is today news')
            sqlList.append(sql)
            aramList.append(params)

    except Exception as e:
        print(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

    time.sleep(1)  # polite crawling (1ì´ˆ ëŒ€ê¸°)
result = updateDB(sqlList, paramList)
if result['status'] == "success":
    print("done!")